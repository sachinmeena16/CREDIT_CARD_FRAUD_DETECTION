# -*- coding: utf-8 -*-
"""credit_card_fraud_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EHFSczQ8vFBp4eC0zQD01IRUYHdYuQDd

Greetings from Wids iitb in this week we start to read the data
"""

## import libraries
import numpy as np
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(style="darkgrid")
import sys

## import data
df=pd.read_csv("/content/creditcard.csv")
## check first 5 rows of data set
df.head()

"""The dataset that we have today consists of 31 variables, including our target variable: Class. 0 represents not fraud and 1 represents fraud. The remaining variables consist of Time, Amount and 28 V variables. The V variables are the result of a PCA (dimensionality reduction) transformation in order to protect customer information. So in this project, we wouldn't have the details to present a very interpretable result. Rather, we'll focus on the methods and evaluations we can adopt to deal with imbalanced datasets.

EDA
"""

sns.countplot(x=df["Class"])

df["Class"].value_counts()

"""we can see that imbalanced data in target variable, Obviously, most credit card transactions are not going to be fraud.

Time and Amount
"""

sns.histplot(data=df, x="Time")

sns.histplot(data=df, x="Amount", bins=500)

### describe the dataset
df.describe()

### information of the dataset
df.info()

## shape of the data set
df.shape

#correlation 
df.corr()

df.nunique()

"""these are the unique values of the each column

Dropping duplicate values
"""

df.duplicated().sum()

df.drop_duplicates(keep='first',inplace=True)

## null values rows drop
df.dropna(how='any',inplace=True)
df

df.isnull().values.any()

"""Splitting the data into Train and Test data.

**Oversampline, undersampling,SMOTE**
now lets move on to dealing with imbalanced data, the three most popular ways to treat the imbalance data are , Oversampling,Undersampling,and Smote. we will demonstrate and try all three to compare which will work in best scenario.
"""

from sklearn.model_selection import train_test_split
y = df.Class
X = df.drop('Class', axis=1)

#split so that the proportions of imbalance remains the same in the training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=42, stratify=y)

"""Here,We'll try to split the training and testing dataset such that the proportions of class 1 and class 0 are the same in both the training and testing dataset."""

#training dataset proportion
y_train.value_counts(normalize=True)

#testing dataset proportion
y_test.value_counts(normalize=True)

"""When we resample our data, remember it's important that you only resample the training data and not the testing data. Build your model using the resampled data and then test it with the untouched testing data."""

from sklearn.utils import resample

## check the counts of 0 and 1s in training data
y_train.value_counts()

## concatenate  our traing data back together for oversampling 
X_concat = pd.concat([X_train, y_train], axis=1)

# separate minority and majority classes
not_fraud = X_concat[X_concat.Class==0]
fraud = X_concat[X_concat.Class==1]

"""**Oversampling**"""

## oversampling fraud transaction minority class
df_fraud_oversampled = resample(fraud, replace=True, n_samples=len(not_fraud), random_state=42) # with replacement

# combine majority and oversampled minority
oversampled = pd.concat([not_fraud, df_fraud_oversampled])

#split X and y
y_train_oversampled = oversampled.Class
X_train_oversampled = oversampled.drop('Class', axis=1)

# check new class counts
oversampled.Class.value_counts()

"""**Undersampling **"""

## we do undersample on majority data , here not fraud is majority data
df_not_fraud_undersampled =resample(not_fraud,replace=False,n_samples=len(fraud),random_state=42)  #without replacement

# combine undersampled majority and minority
undersampled = pd.concat([df_not_fraud_undersampled,fraud])

#split X and y
y_train_undersampled =undersampled.Class
X_train_undersampled =undersampled.drop('Class',axis=1)

# check new class counts
undersampled.Class.value_counts()

"""**SMOTE**"""

from imblearn.over_sampling import SMOTE
sm=SMOTE(random_state=42)
X_train_sm,y_train_sm=sm.fit_resample(X_train,y_train)
y_train_sm.value_counts()

"""**MODEL BUILDING**

**OVERSAMPLING**

Random forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, f1_score

#RFM with oversampled data 
rfm_OS = RandomForestClassifier(random_state=42)

rfm_OS.fit(X_train_oversampled, y_train_oversampled)
rfm_OS_pred = rfm_OS.predict(X_test)

print(classification_report(y_test, rfm_OS_pred))

"""decision tree """

from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier

# Create Decision Tree classifier object
ov_clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
ov_clf = ov_clf.fit(X_train_oversampled,y_train_oversampled)
#Predict the response for test dataset
d_y_pred = ov_clf.predict(X_test)

print(classification_report(y_test,d_y_pred))

"""**UNDERSAMPLING**"""

#RFM with undersampled data
rfm_US = RandomForestClassifier(random_state=42)

rfm_US.fit(X_train_undersampled, y_train_undersampled)
rfm_US_pred = rfm_US.predict(X_test)

print(classification_report(y_test, rfm_US_pred))

"""decision tree"""

# Create Decision Tree classifier object
un_clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
uv_clf = un_clf.fit(X_train_undersampled,y_train_undersampled)
#Predict the response for test dataset
u_y_pred = un_clf.predict(X_test)

print(classification_report(y_test, u_y_pred))

"""****

**SMOTE**
"""

#RFM with SMOTE
rfm_sm = RandomForestClassifier(random_state=42)

rfm_sm.fit(X_train_sm, y_train_sm)
rfm_sm_pred = rfm_sm.predict(X_test)

print(classification_report(y_test,rfm_sm_pred))

"""decision tree"""

# Create Decision Tree classifier object
sm_clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
sm_clf = sm_clf.fit(X_train_sm,y_train_sm)
#Predict the response for test dataset
sm_y_pred = sm_clf.predict(X_test)

print(classification_report(y_test,sm_y_pred))

"""**MODEL COMPARISON**"""

from sklearn.metrics import precision_recall_fscore_support

#model dictionary
predicted_values = { "RandomForest OS": rfm_OS_pred, "RandomForest US": rfm_US_pred, "RandomForest SMOTE": rfm_sm_pred,"DecisionTree OS":d_y_pred,"DecisionTree US":u_y_pred,"DecisionTree SMOTE":sm_y_pred}
#create df
df = pd.DataFrame(columns=['model', 'precision', 'recall', 'f1_score'])
#plug precision recall and f1score into the dataframe for each model
for key, value in predicted_values.items():
    precision = precision_recall_fscore_support(y_test, value, average=None)[0][1]
    recall = precision_recall_fscore_support(y_test, value, average=None)[1][1]
    f1_score = precision_recall_fscore_support(y_test, value, average=None)[2][1]
    df = df.append({'model': key, 'precision':precision, 'recall':recall, 'f1_score':f1_score}, ignore_index=True)

df = df.set_index('model')
df

"""Here we can see that  RandomForest SMOTE give highest value of f1_score,precision,recall  compared two all models then we can say that RandomForest SMOTE is good model for this credit card fraud ditection."""